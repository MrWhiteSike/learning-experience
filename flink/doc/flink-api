flink api 

source :
1）从集合读取数据
2）从文件读取数据
3）从kafka读取数据
4）自定义source
用于测试，或者从其他数据源进行数据读取


transform 算子：

单流转换算子：
1）map
2）flatmap
3）filter

4）keyby
5）滚动聚合算子
	sum
	min
	max
	minBy
	maxBy
6）reduce

多流转换算子：
7）split 和 select
8）connect 和 Comap
9）Union


10）数据重分区操作：
broadcast 广播，每个分区都分发一份
shuffle 随机 类似发牌
forward 直通
rebalance 轮询 不同分区之间数据传输时，默认的传输方式
rescale 重新平衡：分区先进行分组，然后在分区组内进行轮询
global 汇总数据到第一个分区，谨慎使用，因为不能并行执行
partitionCustom 自定义分区




支持的数据类型：
1)基础数据类型
支持所有的Java和Scala基础数据类型，int double long float boolean String 

2)java和Scala元组（Tuples）

3）Scala样例类（ case classes）

4）Java简单对象（POJOs）
必须要有空构造器，

5）其他：Arrays Lists Maps Enums


实现UDF函数--更细粒度的控制流

1)自定义函数类，通用的时候使用，更加灵活
2）匿名函数，lambda 表达式
	注意：泛型擦除
3）富函数
DataStream API提供的一个函数类的接口。
所有Flink函数类都有其Rich版本，与常规函数的不同在于，可以获取运行环境的上下文，并拥有一些生命周期方法，所以可以实现更复杂的功能。
有一个生命周期的概念，典型的生命周期方法有：

open 方法：是rich function的初始化方法，当一个算子例如map，filter被调用之前open会先被调用。
close 方法：生命周期中的最后一个调用的方法，做一些清理工作。



Sink：
Flink没有类似于spark中foreach方法，让用户进行迭代的操作。所有对外的输出操作都要利用Sink完成，最后通过类似如下方式完成整个任务最终输出操作。
官方提供了一部分的框架的sink，除此之外，需要用户自定义实现Sink。








