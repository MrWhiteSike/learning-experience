Spark-core-SourceCode


1、环境准备（Yarn集群）
	1）Driver ，Executor
2、组件通信
	1) Driver => Executor 
	2) Executor => Driver
	3) Executor => Executor
3、应用程序的执行
	1) RDD 依赖
	2) 阶段的划分
	3) 任务的切分
	4) 任务的调度
	5) 任务的执行
4、Shuffle
	1) Shuffle的原理和执行过程
	2) Shuffle写磁盘
	3) Shuffle读取磁盘
5、内存的管理
	1) 内存的分类
	2) 内存的配置



1、环境准备（Yarn集群）

java org.apache.spark.deploy.SparkSubmit
/bin/java org.apache.spark.deploy.yarn.ApplicationMaster 又启动一个 ApplicationMaster 进程
Driver ： runDriver() --> userThread = new Thread （Driver）--> SparkContext

Executor： 

// 处理可用于分配的容器
handleAllocatedContainers
// 运行已分配容器
runAllocatedContainers

// 准备指令：
prepareCommand

// 向指定NM启动容器
nmClient.startContainer


/bin/java org.apache.spark.executor.YarnCoarseGrainedExecutorBackend








JVM ==> Process(SparkSubmit) 就是启动一个进程

进程的起点：SparkSubmit.main

	1、parseArguments：解析提交参数
		--> new SparkSubmitArguments(args)
		--> parse(args.asJava) 利用的是正则表达式 匹配获取 参数和值
		--> handle(opt: String, value: String) 对获取的 参数和值 进行处理
		--> action = Option(action).getOrElse(SUBMIT) 默认为 submit
		--> validateSubmitArguments() 对提交参数进行安全校验
	2、submit(appArgs, uninitLog)
		--> doRunMain()
		--> runMain(args, uninitLog)

			--> (childArgs, childClasspath, sparkConf, childMainClass) = prepareSubmitEnvironment(args)

				--> childMainClass = org.apache.spark.deploy.yarn.YarnClusterApplication
				--> new Client(new ClientArguments(args), conf, null)
					--> YarnClient = new YarnClientImpl() --> ApplicationClientProtocol rmClient
				--> run()
					--> submitApplication()
						--> launcherBackend.connect()
      					-->	yarnClient.init(hadoopConf)
      					-->	yarnClient.start() yarn客户端启动，相当于建立了和yarn集群的连接
      					--> newApp = yarnClient.createApplication()
      					newAppResponse = newApp.getNewApplicationResponse() 从RM中获取一个新应用
      					--> containerContext = createContainerLaunchContext(newAppResponse) 创建容器启动环境，在Yarn当中用来解耦合的，主要配置一些JVM参数以及发送 Java 指令到 NodeManager 去启动  ApplicationMaster.main 进程
      					--> appContext = createApplicationSubmissionContext(newApp, containerContext) 创建应用提交环境
      					--> yarnClient.submitApplication(appContext) 提交应用

			--> app: SparkApplication 生成SparkApplication对象
			--> app.start(childArgs.toArray, sparkConf) SparkApplication对象 调用 start方法，启动应用程序



ApplicationMaster.main

	--> amArgs = new ApplicationMasterArguments(args) 解析参数
	--> sparkConf = new SparkConf() 创建SparkConf对象
		--> sparkConf.set(k, v)
	--> yarnConf = new YarnConfiguration(SparkHadoopUtil.newConfiguration(sparkConf))  创建YarnConfiguration对象

	--> master = new ApplicationMaster(amArgs, sparkConf, yarnConf) 创建ApplicationMaster对象
		--> client = new YarnRMClient()
	--> master.run()
		--> runDriver()
			--> userClassThread = startUserApplication() 启动用户应用程序
				--> mainMethod = userClassLoader.loadClass(args.userClass)
	      .getMethod("main", classOf[Array[String]]) 获取主线程
	      		--> userThread = new Thread ==> 这个线程就是 Driver，其中创建 SparkContext
			--> ThreadUtils.awaitResult(sparkContextPromise.future,
	        Duration(totalWaitTime, TimeUnit.MILLISECONDS)) 线程的阻塞功能，等待SparkContext的创建，等待成功程序就可以继续往下走了

	        --> registerAM(host, port, userConf, sc.ui.map(_.webUrl), appAttemptId) 注册AM ，向ResourceManager申请资源
	        createAllocator(driverRef, userConf, rpcEnv, appAttemptId, distCacheConf) 返回可用资源列表
	        	--> allocatedContainers 
	        	--> handleAllocatedContainers(allocatedContainers) 
	        	处理可分配的容器资源
	        		--> runAllocatedContainers 运行已分配容器
	        			--> launcherPool 线程池启动 Executor 线程（ExecutorRunnable）
	        				--> ExecutorRunnable
		        				--> nmClient
		        				--> startContainer
			        				--> prepareCommand 准备指令，
			        				--> nmClient.startContainer 向指定NM启动容器
			        					--> 执行bin/java 命令执行 YarnCoarseGrainedExecutorBackend.main 启动一个进程


YarnCoarseGrainedExecutorBackend.main
	--> backendArgs = CoarseGrainedExecutorBackend.parseArguments
	--> CoarseGrainedExecutorBackend.run(backendArgs, createFn) 
		--> fetcher
		--> driver
		--> driverConf
		--> env = SparkEnv.createExecutorEnv 
			--> env.rpcEnv.setupEndpoint
				--> dispatcher.registerRpcEndpoint(name, endpoint)
					--> RpcEndpointAddress 通信地址
					--> NettyRpcEndpointRef 通信引用
					--> DedicatedMessageLoop
						--> inbox = new Inbox 
							--> messages.add(OnStart) 
								OnStart
								--> driver = Some(ref) ref.ask 发送RegisterExecutor请求 
								--> CoarseGrainedSchedulerBackend.receiveAndReply 接收RegisterExecutor，并回复true：注册成功
								--> self.send(RegisteredExecutor) Executor就给自身发送一个RegisteredExecutor消息，
								--> receive : 接收到RegisteredExecutor，注册成功
								--> executor = new Executor 创建 Executor对象
								--> driver.get.send(LaunchedExecutor(executorId)) 启动Executor计算资源

						--> threadpool
						--> threadpool.submit(receiveLoopRunnable)

ApplicationMaster：
主要有两条线：
1、跟资源相关：资源的申请：Driver和Executor的资源
2、跟计算相关：让Driver继续执行
resumeDriver()： 让Driver继续执行，Driver是线程，就是让线程继续执行，就是让我们写的程序继续执行
大致过程：
首先在我们自己的应用程序中，
		--> 创建SparkContext对象
			--> postStartHook 
				--> ApplicationMaster.sparkContextInitialized(sc) 完成环境初始化，就可以让我们的程序继续往下执行
				--> super.postStartHook()
					--> while (!backend.isReady) while 循环，有一个状态的等待，等待通知

		--> ApplicationMaster.resumeDriver() 
			--> sparkContextPromise.notify() 进行通知，通知后上述的while程序继续往下执行

最终 资源和计算 合二为一来实现业务功能

概念解释：
rpcEnv ：通信环境
Backend：后台
Endpoint: 终端

RpcEndpoint : 通信终端
它的生命周期 ：constructor -> onStart -> receive* -> onStop



2、组件通信
RpcEnv --> NettyRpcEnvFactory

Netty : 通信框架，支持 AIO

BIO ： 阻塞式IO
NIO ： 非阻塞式IO
AIO ： 异步非阻塞式IO

Linux 对 AIO支持不够好，windows支持

Linux 采用Epoll方式模仿AIO操作


Driver: NettyRpcEnv 
	TransportServer:通信服务器  RpcEndpoint（receive*）：通信终端，作用用来接收数据
		DedicatedMessageLoop.Inbox ：收件箱，所有数据都在inbox中
	RpcEndpointRef（ask*） ：发送数据 
		Outbox：发件箱，可能有多个TransportClient，和Executor的TransportServer进行连接

Executor：NettyRpcEnv
	TransportServer:通信服务器  
	RpcEndpointRef（ask*） ：发送数据 
		Outbox：发件箱 ，可能有多个TransportClient，和Driver的TransportServer进行连接
	RpcEndpoint（receive*）：通信终端，作用用来接收数据
		DedicatedMessageLoop.Inbox ：收件箱，所有数据都在inbox中
	


	SparkEnv.createDriverEnv 
	--> rpcEnv = RpcEnv.create
	--> NettyRpcEnvFactory().create
	--> NettyRpcEnv 
	--> Utils.startServiceOnPort 就是根据端口创建server
		--> TransportServer
		--> NettyUtils.getServerChannelClass
		--> case EPOLL: return EpollServerSocketChannel.class;
	--> dispatcher.registerRpcEndpoint
Executor



3、应用程序的执行

	核心对象：
	SparkContext
		- SparkConf 配置对象 -- 基础环境配置
		- SparkEnv 环境对象 -- 通信环境
		- SchedulerBackend 通信后台 -- 主要用于和 Executor 之间进行通信
		- TaskScheduler 任务调度器 -- 主要用于任务的调度
		- DAGScheduler 阶段调度器 -- 主要用于阶段的划分以及任务的切分

	1) RDD 依赖
	new MapPartitionsRDD (textFile) <-OneToOneDependency- new MapPartitionsRDD (flatMap) <-ShuffleDependency-  ShuffledRDD(groupBy)


	2) 阶段的划分
	submitJob
	// 创建结果阶段
	--> createResultStage 
		// 获取或创建上级阶段
		--> getOrCreateParentStages
			// 如果所依赖的 RDD 不是 ShuffledRDD，就获取或创建 ShuffleMap 阶段 
			--> getOrCreateShuffleMapStage
	--> new ResultStage
	
	总结：
	1. 划分阶段个数 = Shuffled依赖个数 + 1
	2. Shuffle 需要落盘：ShuffleMapStage 写磁盘，ResultStage 读磁盘

	3) 任务的切分

	submitJob
	--> eventProcessLoop.post(JobSubmitted)
	--> handleJobSubmitted
		--> submitStage
			--> if (missing.isEmpty) submitMissingTasks
				--> partitionsToCompute = findMissingPartitions
				--> tasks = new ShuffleMapTask
			-->  submitStage(parent)

	总结：
	总的任务数量 = 所有阶段的每个阶段的最后一个RDD的分区数量之和

	4) 任务的调度
	TaskSet 一个Task数据集
	taskScheduler.submitTasks(new TaskSet）
		--> createTaskSetManager
			--> new TaskSetManager
		// default scheduler is FIFO
		--> schedulableBuilder.addTaskSetManager
			schedulableBuilder: FIFOSchedulableBuilder,FairSchedulableBuilder
		--> rootPool.addSchedulable(manager) 往任务池放任务
			任务池
		--> backend.reviveOffers() 从任务池中取任务
		 	--> receive
		 		--> makeOffers()
		 				// in a round-robin manner 以轮询的方式来取任务
		 			--> scheduler.resourceOffers(workOffers) 
		 				--> getSortedTaskSetQueue 根据调度算法进行排序
		 					taskSetSchedulingAlgorithm : FairSchedulingAlgorithm; FIFOSchedulingAlgorithm 调度算法
		 				--> for (taskSet <- sortedTaskSets)  对任务进行轮询
		 				// 移动数据不如移动计算
		 				// 计算和数据的位置存在不同的级别，这个级别称之为本地化级别
		 				// 进程本地化：数据和计算在同一个进程中 （级别最快）
		 				// 节点本地化：数据和计算在同一个节点中
		 				// 机架本地化：数据和计算在同一个机架中
		 				// 任意位置
		 				--> for (currentMaxLocality <- taskSet.myLocalityLevels)  首选位置的概念
		 			--> launchTasks(taskDescs) 启动任务
		 				--> executorData.executorEndpoint.send(LaunchTask(new SerializableBuffer(serializedTask))) 把任务池当中任务序列化后发送到某个远程的 Executor 终端(总的来说，就是 SchedulerBackend 发消息)

	5) 任务的执行
	ExecutorBackend 接收消息：
	receive
	--> LaunchTask
		--> taskDesc = TaskDescription.decode(data.value) 消息反序列化
		--> executor.launchTask(this, taskDesc) 启动任务执行
			--> new TaskRunner 创建任务线程
				--> task.run
				--> runTask(context) 表示运行任务，它是一个抽象方法，意味着每个任务都要重写规则
			--> threadPool.execute(tr) 把任务线程放到线程池中去执行

	总结：
	计算任务是我们的计算对象来运行的，但是每个运算逻辑在每个任务当中


Spark任务调度概述

当Driver起来后，Driver则会根据用户程序逻辑准备任务，并根据Executor资源情况逐步分发任务

几个概念：Job，stage，task
Job ：以action算子为界，遇到一个action方法则触发一个Job
stage ：是Job的子集，以RDD宽依赖即（Shuffle）为界，遇到Shuffle做一次划分
Task：是stage 的子集，以并行度（分区数）来衡量，一个阶段的最后一个RDD的分区数是多少，则有多少个task

Spark的任务调度总体来说分两路进行，一路是Stage级的调度，一路是Task级的调度，总体调度流程




4、Shuffle

Shuffle 一定会有落盘
如果Shuffle过程中落盘数据量减少，那么可以提高性能。
算子如果存在预聚合功能，可以提高Shuffle的性能

1) Shuffle的原理和执行过程
 shuffleWriterProcessor ==> 写处理器
 ShuffleManager (trait) ==> 只有一个SortShuffleManager实现


SortShuffleManager
getWriter

	case：
	处理器
		写对象
		判断条件

		 SerializedShuffleHandle
		 	UnsafeShuffleWriter
		 	1.序列化规则支持重定位操作（Java序列化不支持，Kyro支持）
		 	2.不能使用预聚合
		 	3.如果下游的分区数量小于或者等于16777216

		 BypassMergeSortShuffleHandle
		 	BypassMergeSortShuffleWriter
		 	1.不能使用预聚合
		 	2.如果下游的分区数量小于等于200（可配）

		 BaseShuffleHandle
		 	SortShuffleWriter
		 	其他情况


SortShuffleWriter
 sorter 排序器



5、内存的管理

3.0之前 静态内存管理
3.0之后 统一内存管理（动态内存管理）
	1) 内存的分类
	存储内存：缓存数据，广播变量  30%
	执行内存：Shuffle过程中操作  30%
	其他内存：系统，RDD元数据信息 40%
	预留内存：300M



	2) 内存的配置








