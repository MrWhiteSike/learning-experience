spark-cluster

Spark 集群环境搭建 -- 部署Spark集群

standalone模式：

1、下载
https://archive.apache.org/dist/spark/spark-3.0.0/

2、上传并解压
tar -zxvf spark-3.0.0-bin-hadoop3.2.tgz

3、配置SPARK_HOME环境变量
vim ~/.bashrc 
添加如下内容：
export SPARK_HOME=/opt/module/spark-3.0.0-bin-hadoop3.2
export PATH=$PATH:$SPARK_HOME/bin:$SPARK_HOME/sbin
加载使其生效：source ~/.bashrc 

4、修改配置
cd /opt/module/spark-3.0.0-bin-hadoop3.2/conf

cp spark-defaults.conf.template spark-defaults.conf
cp spark-env.sh.template spark-env.sh
cp slaves.template slaves

修改slaves文件，添加从机
vim slaves
修改为
hadoop1
hadoop2
hadoop3

修改spark-defaults.conf
添加如下内容：
spark.master                     spark://hadoop1:7077
spark.eventLog.enabled           true
spark.eventLog.dir               hdfs://hadoop1:8020/spark-history
spark.serializer                 org.apache.spark.serializer.KryoSerializer
spark.driver.memory              1g
spark.executor.memory            1g

修改spark-env.sh
export JAVA_HOME=/usr/java/jdk1.8.0_131
export HADOOP_HOME=/opt/module/hadoop
export HADOOP_CONF_DIR=/opt/module/hadoop/etc/hadoop
export SPARK_DIST_CLASSPATH=$(/opt/module/hadoop/bin/hadoop classpath)
export SPARK_MASTER_HOST=hadoop1
export SPARK_MASTER_PORT=7077


5、将spark-3.0.0-bin-hadoop3.2 目录分发到其他节点
scp -r ./spark-3.0.0-bin-hadoop3.2 hadoop2:/opt/module/
scp -r ./spark-3.0.0-bin-hadoop3.2 hadoop3:/opt/module/

6、启动spark集群
cd /opt/module/spark-3.0.0-bin-hadoop3.2/
./sbin/start-all.sh

7、在web界面查看spark ui：http://hadoop1:8080/

8、测试：
运行SparkPI 案例测试


Yarn模式
上面默认是用standalone模式启动的服务，如果想要把资源调度交给yarn来做，则需要配置为yarn模式：
需要启动的服务：hdfs服务、yarn服务
需要关闭 Standalone 对应的服务(即集群中的Master、Worker进程)

在Yarn模式中，Spark应用程序有两种运行模式：
yarn-client。Driver程序运行在客户端，适用于交互、调试，希望立即看到app的输出
yarn-cluster。Driver程序运行在由RM启动的 AppMaster中，适用于生产环境
二者的主要区别：Driver在哪里

1、关闭 Standalone 模式下对应的服务；开启 hdfs、yarn服务
2、修改 yarn-site.xml 配置
在 $HADOOP_HOME/etc/hadoop/yarn-site.xml 中增加，分发到集群，重启 yarn 服务
# vim /opt/module/hadoop/etc/hadoop/yarn-site.xml 
<property>
        <name>yarn.nodemanager.pmem-check-enabled</name>
        <value>false</value>
</property>
<property>
        <name>yarn.nodemanager.vmem-check-enabled</name>
        <value>false</value>
</property>
备注：
yarn.nodemanager.pmem-check-enabled。是否启动一个线程检查每个任务正使用的物理内存量，如果任务超出分配值，则直接将其杀掉，默认是true

yarn.nodemanager.vmem-check-enabled。是否启动一个线程检查每个任务正使用的虚拟内存量，如果任务超出分配值，则直接将其杀掉，默认是true

3、修改配置，分发到集群
# spark-env.sh 中这一项必须要有
# cd /opt/module/spark/conf
export HADOOP_CONF_DIR=/opt/module/hadoop/etc/hadoop

# spark-default.conf(以下是优化)
# 与 hadoop historyserver集成
# vim spark-defaults.conf
spark.yarn.historyServer.address   dev-spark-master-206:18080
spark.yarn.jars                    hdfs://hadoop1:8020/spark-jars/*.jar

4、向hdfs上传spark纯净版jar包
说明：
1）直接使用会和安装的Hive3.1.2出现兼容性问题。所以采用Spark纯净版jar包，不包含hadoop和hive相关依赖，避免冲突
2）Hive任务最终由spark来执行，spark任务资源分配由Yarn来调度，该任务有可能被分配到集群的任何一个节点。所以需要将spark的依赖上传到HDFS集群路径，这样集群中任何一个节点都能获取到

上传并解压spark-3.0.0-bin-without-hadoop.tgz
tar -zxvf spark-3.0.0-bin-without-hadoop.tgz

上传spark纯净版jar包到hdfs
hdfs dfs -mkdir /spark-jars
hdfs dfs -put spark-3.0.0-bin-without-hadoop/jars/* /spark-jars

5、测试
记得，先把Master与worker进程停掉，否则会走standalone模式
# 停掉standalone模式
stop-all.sh

client模式测试
# client
spark-submit --master yarn \
--deploy-mode client \
--class org.apache.spark.examples.SparkPi \
$SPARK_HOME/examples/jars/spark-examples_2.12-3.0.0.jar 2000
在提取App节点上可以看见：SparkSubmit、YarnCoarseGrainedExecutorBackend
在集群的其他节点上可以看见：YarnCoarseGrainedExecutorBackend
在提取App节点上可以看见：程序计算的结果（即可以看见计算返回的结果）

cluster模式测试
# cluster
spark-submit --master yarn \
--deploy-mode cluster \
--class org.apache.spark.examples.SparkPi \
$SPARK_HOME/examples/jars/spark-examples_2.12-3.0.0.jar 2000

在提取App节点上可以看见：SparkSubmit
在集群的其他节点上可以看见：YarnCoarseGrainedExecutorBackend、ApplicationMaster(Driver运行在此)
在提取App节点上看不见最终的结果
