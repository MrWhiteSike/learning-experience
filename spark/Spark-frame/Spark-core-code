Spark核心编程

Spark计算框架为了能够进行高并发和高吞吐的数据处理，封装了三大数据结构，用于处理不同的应用场景，三大数据结构分别是：
RDD：弹性分布式数据集
累加器：分布式共享 只写 变量
广播变量：分布式共享 只读 变量


RDD，Resilient Distributed Dataset
弹性分布式数据集，是spark中最基本的数据处理模型。最小的数据处理单元。
代码中是一个抽象类，代表一个弹性的、不可变、可分区、里面的元素可并行计算的集合。
是一种数据准备好，逻辑准备好的数据结构。组织、存储的结构

弹性：
存储的弹性：内存与磁盘的自动切换
容错的弹性：数据丢失可以自动恢复
计算的弹性：计算出错重试机制
分片的弹性：可根据需要重新分片

在Driver中把RDD分解成不同的Task发给Executor去执行计算。


################### IO模型类比RDD ###################

IO模型中：
字节流：提升性能的方式是：添加一个缓冲区buffer，当数据达到阈值时，才会输出到下游。
字符流：一行一行读，需要用到字节流转字符流的转换流InputStreamReader/OutputStreamWriter

装饰者设计模式，核心没变，在外层进行了包装，在之前的基础上扩展更强大的功能。
FileInputStream 读取字节-- InputStreamReader 字节转字符 -- BufferedFileReader 缓存字符
逐层功能的叠加
延迟加载的感觉


WordCount：

file --> HadoopRDD-textFile --> MapPartitionsRDD-flatMap --> MapPartitionsRDD-map --> ShuffledRDD-reduceByKey --> collect --> console

RDD与IO流的异同：

RDD的数据处理方式类似于IO流，也有装饰者设计模式。
RDD的数据只有在调用collect方法时，才会真正执行业务逻辑操作，之前的封装全部都是功能的扩展。
RDD是不保存数据的，但是IO可以临时保存一部分数据。


分布式：数据存储在大数据集群不同节点上
数据集：RDD封装了计算逻辑，并不保存数据
数据抽象：RDD是一个抽象类，需要子类具体实现
不可变：RDD封装了计算逻辑，是不可以改变的，想要改变，只能产生新的的RDD，在新的RDD里面封装计算逻辑
可分区、并行计算

RDD 核心属性
1、分区列表
RDD数据结构中存在分区列表，用于执行任务时并行计算，是实现分布式计算的重要属性。
getPartitions

2、分区计算函数
Spark在计算时，是使用分区函数对每一个分区进行计算
compute

3、RDD之间的依赖关系
RDD是计算模型的封装，当需求中需要将多个计算模型进行组合时，就需要将多个RDD建立依赖关系，就会形成一个列表
getDependices

4、分区器Partitioner
分区规则

5、首选位置
判断计算发送到哪个节点，效率最优
getPreferredLocations

移动数据不如移动计算。


################### 执行原理 ###################

执行原理
从计算角度来讲，数据处理过程需要计算资源（内存 & CPU）和计算模型（逻辑）。执行时，需要将计算资源和计算模型进行协调和整合。

Spark 框架在执行时，先申请资源，然后将应用程序的数据处理逻辑分解成一个一个计算任务，然后将任务发到已经分配好资源的计算节点上，按照指定的计算模型进行数据计算。最后得到计算结果。


RDD是Spark框架中用于数据处理的核心模型。在Yarn环境中，RDD的工作原理：

RDD在整个流程中主要用于将逻辑进行封装，并生成Task发送给Executor节点执行计算。


################### RDD 创建方式 ###################

RDD创建：
1、从集合（内存）中创建
从集合中创建RDD，Spark主要提供了两个方法：parallelize和makeRDD
2、从外部存储（文件）创建RDD
textFile : 以行为单位来读取数据
wholeTextFiles ： 以文件为单位读取数据
    读取的结果表示为元组，第一个参数表示文件路径，第二个参数为文件内容
注意：path参数
1、path路径默认以当前环境的根路径为基准。可以写绝对路径，也可以写相对路径
2、path路径可以是文件的具体路径，也可以是目录名称
3、path路径还可以使用通配符 *
4、path路径还可以是分布式存储系统路径：HDFS
3、从其他框架创建
4、new创建



RDD并行度与分区：
默认情况下，Spark可以将一个作业切分为多个任务，发送给Executor节点进行计算，而能够并行计算的任务数量我们称之为并行度。
这个数量可以在构建RDD时指定。记住，这里的并行执行的任务数量，并不是指的切分任务的数量。

例如，makeRDD方法
1、可以传递第二个参数，这个参数表示分区的数量
2、第二个参数是可以不传递，会使用默认值：defaultParallelism
Spark在默认情况下，从配置对象中获取配置参数:spark.default.parallelism
如果获取不到，那么使用totalCores属性，这个属性取值为当前运行环境的最大可用核数。

分区源码解析：
 1、调用了 ParallelCollectionRDD.slice[T : ClassTag](seq: Seq[T], numSlices: Int): Seq[Seq[T]] 方法
 2、接着调用def positions(length: Long, numSlices: Int): Iterator[(Int, Int)] 方法，通过分区数获取每个分区在seq中的（start，end）元素位置
 3、然后通过（start，end）切割出seq中对应位置的元素，完成分区操作



 读取文件数据时，数据是按照Hadoop文件读取的规则进行切片分区，而切片规则和数据读取的规则有些差异：
textFile : 可以将文件作为数据处理的数据源，默认也可以设定分区。
      minPartitions ： 最小分区数量
      math.min(defaultParallelism, 2) 默认 2 个分区
如果不想使用默认的分区数量，可以通过第二个参数指定分区数
 注意：Spark 读取文件，底层其实使用的是Hadoop的读取方式
 分区数量的计算方式：
 	  假设要读取文件的大小为23byte，指定分区数为3时，即：
      totalSize = 23
      计算规则：
      goalSize = 23 / 3 = 7 (byte)
      23 / 7 = 3.2..（>10%）+ 1 = 4 (分区) (Hadoop 读取文件时，有个1.1 倍概念：如果剩余的字节数/每个分区字节数 > 10%时，产生一个新分区；如果 < 10%，不会产生新的分区)

分区数据的分配：
  1、数据以行为单位进行读取，和字节数没有关系
  2、数据读取时是以偏移量为单位，偏移量不会被重复读取。
  3、数据分区的偏移量范围的计算：
     goalSize = 23 / 3 = 7 (byte)
     [0,7] 不够一行的，读取整行；
     [7,14] 由于不能重复读取，开始读取下一行，但是数据也是不够一行，也是读取整行；数据读取完了
     [14,21] 该分区没有数据
     [21,23] 该分区没有数据

 如果数据源为多个文件，那么计算分区时以文件为单位进行分区


################### RDD 两种类型的方法：Operator-Transform（转换）和  Operator-Action（行动） ###################


RDD方法分类：两大类
1、转换，功能的补充，将旧的RDD包装成新的RDD
	flatMap, map

2、行动，触发任务的调度和作业的执行
	collect


算子 ： 是为了 区分 RDD的方法和Scala 中集合的方法，因为 RDD的方法名和 Scala集合的方法名 有些是相同，容易造成混淆，故RDD 的方法 起名为算子。


RDD 方法 ==> RDD 算子
认知心理学认为解决问题其实将问题的状态进行改变：
问题（初始）=> 操作（算子） => 问题（审核中）=> 操作（算子） => 问题（完成）

操作：Operator


################### Operator-Transform : ###################

RDD转换算子：
RDD根据数据处理方式的不同将算子整体上分为 Value类型、双Value类型和Key-Value类型

Value 类型 （map, mapPartitions, flatMap, glom, groupBy, filter, sample, distinct, coalesce, repartition, sortBy）

map：将处理的数据逐条进行映射转换，这里的转换可以是类型的转换，也可以是值的转换。

注意：
 1、RDD的计算一个分区内的数据是一个一个执行逻辑
    -- 只有前面一个数据全部的逻辑执行完毕后，才会执行下一个数据
    -- 分区内数据的执行是有序的
 2、不同分区之间数据的执行顺序是无序的


mapPartitions ：可以以分区为单位进行处理，这里的处理是指可以进行任意的处理，哪怕是过滤数据
      但是，会将整个分区的数据加载到内存进行引用
      如果处理完的数据是不会被释放的，存在对象的引用
      在内存较小，数据量较大的场合下，容易出现内存溢出

      还可以实现一些特殊功能，比如取出每个分区中的最大值; map中无法实现，因为map没法区分数据属于哪个分区


问题：map和mapPartitions的区别？

1、数据处理角度：Map算子是分区内一个数据一个数据的执行，类似于串行操作。而mapPartitions算子以分区为单位进行批处理操作

2、功能的角度：
map算子主要目的将数据源中的数据进行转换和改变。但是不会减少或增加数据。mapPartitions算子需要传递一个迭代器，返回一个迭代器，没有要求元素的个数保持不变，可以增加或减少数据

3、性能的角度：
Map算子因为类似于串行操作，所以性能比较低，而mapPartitions算子类似于批处理，所以性能比较高。但是mapPartitions算子会长时间占用内存，那么这样会导致内存可能不够用，出现内存溢出错误。所以在内存有限的情况下，不推荐使用mapPartitions，而是使用map操作。

完成比完美更重要。

mapPartitionsWithIndex: 可以以分区为单位发送到计算节点进行处理，这里的处理是指可以进行任意的处理，哪怕是过滤数据，在处理时同时可以获取当前分区索引。

特殊功能：比如，获取某个分区的数据、某个数据对应的所属分区等功能


flatMap：将处理的数据进行扁平化后再进行映射处理，所以算子也称为扁平映射。返回一个可迭代的集合就是满足要求的。

注意：当集合中的数据类型不同时，可以使用match case 进行模式匹配，转换成集合类型返回。



glom：将同一个分区的数据直接转换为相同类型的内存数组进行处理，分区不变。


groupBy：将数据根据指定的规则进行分组，分区默认不变，但是数据会被打乱，我们将这样的操作称为shuffle，极限情况下，数据可能会在同一个分区中。

一个组的数据在一个分区中，但是并不是说一个分区中只有一个组。

filter：将数据根据指定的规则进行筛选过滤，符合规则的数据保留，不符合规则的数据丢弃。
当数据进行筛选过滤后，分区不变，但是分区内数据可能不均衡，生产环境中，可能会出现数据倾斜。


sample：根据指定的规则从数据集中抽取数据
需要传递三个参数:
 第一个参数：表示抽取数据后是否将数据放回 true：放回，false：不放回
 第二个参数：表示数据源中每条数据被抽取的概率
 			基准值的概念
 第三个参数：表示抽取数据时随机算法的种子
 			如果不传递第三个参数，那么使用的是当前系统时间作为随机数的种子

使用场景：
数据倾斜时，通过sample找到倾斜的key，然后针对这个key进行特殊处理，避免数据倾斜。


distinct：将数据集中重复数据去重

内存集合distinct的去重方式使用 HashSet 去重
RDD的distinct底层逻辑：map(x => (x, null)).reduceByKey((x, _) => x, numPartitions).map(_._1)，使用分布式处理方式实现去重

如果不用该算子，有什么办法实现数据去重？
groupBy 分组实现
filter 过滤实现


coalesce: 根据数据量缩减分区，用于大数据集过滤后，提高数据集的执行效率
当Spark程序中存在过多的小任务时，可以通过coalesce方法，收缩合并分区，减少分区个数，减少任务调度成本。

注意：
 1、coalesce方法默认情况下不会将分区数据打乱重新组合
 这种情况下的缩减分区可能会导致数据不均衡，出现数据倾斜
 如果想要让数据均衡，可以进行shuffle处理，添加第二个参数为true就可以了

如何扩大分区呢？

 2、coalesce 算子可以扩大分区的，但是如果不进行shuffle操作，是没有意义的，不起作用的，
 所以如果想要实现扩大分区的效果，需要使用shuffle操作，即设置第二个参数为true

 3、Spark提供了简化的操作
 缩减分区：coalesce，如果想要数据均衡，可以采用shuffle
 扩大分区：repartition 底层代码就是调用 coalesce，而且肯定shuffle


sortBy: 根据指定的规则进行排序，默认为升序，第二个参数可以改变排序方式
		sortBy 默认情况下，不会改变分区，但是中间存在shuffle操作


双 Value 类型： (intersection, union, subtract, zip)

intersection : 对源RDD和参数RDD求交集后返回一个新的RDD

问题：如果两个RDD数据类型不一致怎么办？

union: 	求并集

subtract: 求差集

zip：将两个RDD中的元素，以键值对的形式进行合并。其中，键值对中的key为第一个RDD中的元素，Value为第二个RDD中的相同位置的元素。

注意：
1、交集，并集和差集要求两个数据源数据类型保持一致
2、拉链：两个数据源的数据类型可以不一致，但是，两个数据源的分区数量要保持一致，并且每个分区中数据数量也要保持一致


Key-Value 类型：(partitionBy, reduceByKey, groupByKey, aggregateByKey, foldByKey, combineByKey, join, leftOuterJoin, rightOuterJoin, cogroup)


partitionBy：将数据按照指定Partitioner重新进行分区。
			Spark默认的分区器是HashPartitioner
问题1: 如果重分区的分区器和当前RDD的分区器一样怎么办？
不会生成新的RDD，还是当前RDD本身

问题2: Spark还有其他分区器吗？
HashPartitioner 默认情况下的分区器
RangePartitioner 排序的场景下用的比较多
PythonPartitioner 特定包下才可以用的

问题3:如果想按照自己的方法进行数据分区怎么办？
自定义分区器来改变数据存放的位置


reduceByKey ： 可以将数据按照相同的Key对Value进行聚合

注意：
	1、scala 语言中一般的聚合都是两两聚合，spark基于scala开发的，所以它的聚合也是两两聚合的
	2、如果key的数据只有一个，是不会参与运算的。

groupByKey： 将数据源中的数据，相同的key的数据分到一个组中，形成一个对偶元组
			元组中的第一个元素就是key，第二个元素就是相同key的value集合


问题：reduceByKey 和 groupByKey 的区别？

groupByKey 会导致数据打乱重组，存在shuffle操作。

Spark 中，shuffle操作必须落盘处理，不能在内存中数据等待，会导致内存溢出。shuffle操作的性能比较低。

reduceByKey ：支持分区内预聚合（combine）功能，可以有效减少shuffle时落盘的数据量，提升shuffle的性能。
分区内和分区间计算规则是相同的。

从shuffle的角度： reduceByKey 和 groupByKey 都存在shuffle操作，但是reduceByKey可以在shuffle前对分区内相同key的数据进行预聚合（combine）功能，这样会减少落盘的数据量，而 groupByKey 只是进行分组，不存在数据量减少的问题，reduceByKey 性能更高
从功能的角度： reduceByKey 其实包含分组和聚合的功能。 groupByKey 只能分组，不能聚合，所以在分组聚合的场合下，推荐使用 reduceByKey ，如果仅仅是分组而不需要聚合。那么还是只能使用 groupByKey



aggregateByKey：可以根据不同的规则进行分区内计算和分区间计算。存在函数柯里化，有两个参数列表
 第一个参数列表:需要传递一个参数，表示为初始值
      主要用于当碰见第一个key的时候，和value进行分区内计算
 第二个参数列表：
      第一个参数表示分区内计算规则
      第二个参数表示分区间计算规则


foldByKey: 如果聚合计算时，分区内和分区间计算规则相同，Spark提供了简化的方法
	第一个参数列表:需要传递一个参数，表示为初始值
      主要用于当碰见第一个key的时候，和value进行分区内计算
    第二个参数列表：
      第一个参数表示分区内和分区间的计算规则


 combineByKey 需要三个参数
	 第一个参数表示：将相同key的第一个数据进行数据结构的转换，实现操作
     第二个参数表示：分区内的计算规则
     第三个参数表示：分区间的计算规则

问题：几种聚合算子的区别？

reduceByKey :
  分区内和分区间计算规则相同
  combineByKeyWithClassTag[V](
  (v: V) =>
   v,   // 第一个值不会参与计算
   func, // 分区内计算规则
   func, // 分区间计算规则
   partitioner)

 aggregateByKey :
  分区内和分区间计算规则不同
  combineByKeyWithClassTag[U](
 (v: V) =>
  cleanedSeqOp(createZero(), v), // 初始值和第一个key的value值进行的分区内数据操作
  cleanedSeqOp, // 分区内计算规则
 combOp, // 分区间计算规则
 partitioner)



 foldByKey :
  分区内和分区间处理函数相同
 combineByKeyWithClassTag[V]((v: V) =>
 cleanedFunc(createZero(), v), // 初始值和第一个key的value值进行的分区内数据操作
 cleanedFunc, // 表示分区内数据的处理函数
 cleanedFunc, // 表示分区间数据的处理函数
 partitioner)


 combineByKey :
  分区内和分区间处理函数不同
 combineByKeyWithClassTag(
 createCombiner, // 相同的key的第一条数据进行的处理函数
 mergeValue, // 表示分区内数据的处理函数
 mergeCombiners, // 表示分区间数据的处理函数
 defaultPartitioner(self))



join ：在类型为（K，V）和（K，W）的RDD上调用，返回一个相同的key对应的所有元素连接在一起的（K，（V，W））的RDD

相同于SQL表中的内连接

问题：如果key存在不相等呢？
注意：
1、如果两个数据源中key没有匹配上，那么数据不会出现在结果中。
2、如果两个数据源中key有多个相同的，会依次匹配，可能会出现笛卡尔积，数据量会几何性增长，会导致性能降低；如果有其他办法代替，最好不要用这个算子。
谨慎使用，如果不用join也能实现同样的功能，那就好了！


leftOuterJoin: 类似于SQL语句的左外连接，左表所有的元素都会出现，右表的匹配元素出现

rightOuterJoin：类似于SQL中右外连接；右表所有的元素都会出现，左表的匹配元素出现



cogroup: connect + group
相同的key，value分组后连接起来；

该算子可以最多连接三个其他的rdd。

################### Operator-Action （行动）: ###################
行动算子，其实就是触发作业执行的方法
底层代码逻辑：调用的是环境对象中 runJob 方法，然后调用dagScheduler（有向无环图）的runJob方法创建ActiveJob，并提交执行。

reduce, collect, count, first, take, takeOrdered, aggregate, fold, 
countByKey, countByValue, saveAsTextFile, saveAsObjectFile, saveAsSequenceFile, foreach


reduce：
聚合RDD中的所有数据，先聚合分区内数据，在聚合分区间数据


collect : 
采集，该方法会将不同分区间的数据按照分区顺序采集到Driver端，形成数组


count : 
数据源中数据个数


first : 
获取数据源中数据的第一个


take : 
获取数据源中数据的前N个

takeOrdered : 
数据先排序，再取N个，默认升序排序，可以使用第二个参数列表（比如 ： Ordering.Int.reverse）实现倒序功能


aggregate : 分区的数据通过 初始值 和分区内的数据进行聚合，然后再和 初始值 进行分区间的数据聚合

aggregateByKey 和 aggregate 的区别？
aggregateByKey ：初始值只会参与分区内计算；转换算子，得到新的RDD
aggregate ：初始值会参与分区内计算，并且还参与分区间计算；行动算子，得到结果


fold ：
折叠，aggregate 简化版，分区内和分区间计算规则相同


countByValue :
统计数据源中value出现的个数（Long），返回一个Map，value值作为Map的key，个数作为Map的value


countByKey :
要求数据为 Key-Value 类型，统计数据源中 key 出现的个数（Long），返回一个Map，数据源中的 key 值作为Map的key，个数作为Map的value


save方法：
saveAsTextFile
saveAsObjectFile
saveAsSequenceFile ： 方法要求数据的格式必须为 K-V 键值对类型


foreach :
分布式遍历 RDD中的每一个元素，调用指定函数。

	总结 
    // 算子 ： Operator （操作）
    //        RDD的方法和Scala集合对象的方法不一样：
    //        集合对象的方法都是在同一个节点的内存中完成的
    //        RDD方法可以将计算逻辑发送到Executor端（分布式节点）执行的
    // 		  为了区分不同的处理效果，所以将RDD方法称之为算子
    //		  RDD 方法外部的操作都是在Driver端执行的，而方法内部的逻辑代码是在Executor端执行的。



RDD 序列化：

1、闭包检测
 从计算角度，算子以外的代码都是在Driver端执行，算子里面的代码都是在Executor端执行。那么在scala的函数式编程中，就会导致算子内经常会用到算子外的数据，这样就会形成闭包效果，如果使用的算子外的数据无法序列化，就意味着无法传值给Executor端执行，就会发生错误，所以需要在执行任务计算前，检测闭包内的对象是否可以进行序列化，这个操作我们称之为 闭包检测。
 Scala2.12版本后闭包编译方式发生了改变。

 数据库的连接是不可以序列化进行传递的。

 2、序列化方法和属性

 3、Kryo序列化框架

 参考地址:https://github.com/EsotericSoftware/kryo
 Java 的序列化能够序列化任何的类，但是 比较重 （字节多），序列化后，对象的提交也比较大。Spark出于性能的考虑，Spark2.0开始支持另外一种Kyro序列化机制。Kyro速度是Serializable的10倍。当RDD在Shuffle数据的时候，简单数据类型、数组和字符串类型已经在Spark内部使用Kyro来序列化。
 注意：即使使用Kyro序列化，也要 extents Serializable接口。



 RDD 依赖关系
 相邻的两个RDD的关系称之为依赖关系
 多个连续的RDD的依赖关系，称之为血缘关系

 RDD不会保存数据的，
 RDD为了提供容错性，需要将RDD间的关系保存下来，一旦出现错误，可以根据血缘关系将数据源重新读取进行计算。


 1、RDD 血缘关系
  RDD 只支持粗粒度转换，即在大量记录上执行的单个操作。将创建RDD的一系列 Lineage （血统）记录下来，以便恢复丢失的分区，RDD的Lineage会记录RDD的元数据信息和转换行为，当该RDD的部分分区数据丢失时，它可以根据这些信息来 重新运算 和 恢复丢失的数据分区。

  可以使用 RDD 的toDebugString 算子查看RDD之间的血缘关系

 2、RDD依赖关系
 相邻的两个RDD的关系称之为依赖关系
 可以使用 RDD 的toDebugString 算子查看RDD之间的血缘关系

 新的RDD的一个分区的数据依赖于旧的RDD一个分区的数据，
 这个依赖称之为OneToOne依赖（窄依赖）

 新的RDD的一个分区的数据依赖于旧的RDD多个分区的数据，
 这个依赖称之为Shuffle依赖（宽依赖）

3、RDD窄依赖
窄依赖 表示每一个父（上游）RDD 的partition 最多被 子（下游） RDD的一个partition使用，窄依赖形象的比喻为独生子女。
class OneToOneDependency[T](rdd: RDD[T]) extends NarrowDependency[T](rdd)

4、RDD宽依赖
宽依赖 表示同一个父（上游）RDD的Partition被多个子（下游）RDD的Partition依赖，会引起Shuffle，总结：宽依赖形象的比喻为多生。

class ShuffleDependency[K: ClassTag, V: ClassTag, C: ClassTag](
    @transient private val _rdd: RDD[_ <: Product2[K, V]],
    val partitioner: Partitioner,
    val serializer: Serializer = SparkEnv.get.serializer,
    val keyOrdering: Option[Ordering[K]] = None,
    val aggregator: Option[Aggregator[K, V, C]] = None,
    val mapSideCombine: Boolean = false,
    val shuffleWriterProcessor: ShuffleWriteProcessor = new ShuffleWriteProcessor)
  extends Dependency[Product2[K, V]]


 5、RDD阶段划分

 DAG 有向无环图 是由点和线组成的拓扑图形，该图形具有方向，不会闭环。
  DAG记录了RDD的转换过程和任务的阶段。

 6、RDD 阶段划分源码

  runJob -> DagScheduler.runJob -> 提交任务事件JobSubmitted -> 处理该事件handleJobSubmitted  -> 划分阶段createResultStage -> 返回阶段对象ResultStage -> 提交阶段submitStage
  -> submitMissingTasks -> partitionsToCompute

当RDD中存在shuffle依赖时，阶段会自动增加一个
阶段的数量= shuffle依赖的数量 + 1

 7、RDD 任务划分 
  RDD 任务切分为：Application 、 Job、Stage 和 Task

  Application ：初始化一个 SparkContext 即生成一个 Application
  Job ： 一个Action 算子就会生成一个Job
  Stage ： Stage 等于宽依赖 的个数 + 1；
  Task ： 一个stage阶段中，最后一个RDD的分区个数就是 Task的个数。

  注意： Application -> Job -> Stage -> Task 每一层都是 1对N 的关系

 8、RDD任务划分源码 

 匹配 ShuffleMapStage 和 ResultStage 是对阶段的划分

 从源码 partitionsToCompute.map 可以看出：任务个数 = 最后一个RDD的分区个数 

 val taskIdToLocations: Map[Int, Seq[TaskLocation]] = try {
      stage match {
        case s: ShuffleMapStage =>
          partitionsToCompute.map { id => (id, getPreferredLocs(stage.rdd, id))}.toMap
        case s: ResultStage =>
          partitionsToCompute.map { id =>
            val p = s.partitions(id)
            (id, getPreferredLocs(stage.rdd, p))
          }.toMap
      }
    } catch {
      case NonFatal(e) =>
        stage.makeNewStageAttempt(partitionsToCompute.size)
        listenerBus.post(SparkListenerStageSubmitted(stage.latestInfo, properties))
        abortStage(stage, s"Task creation failed: $e\n${Utils.exceptionString(e)}", Some(e))
        runningStages -= stage
        return
    }


 RDD 持久化 


 	总结：
	RDD 中不存储数据
	如果一个RDD需要重复使用，那么需要从头再次执行来获取数据
	RDD对象是可以重用的，但是数据无法重用

 持久化 ：可以放到文件（persist 算子）中，也可以放到内存(cache 算子)中 

 cache 默认持久化的操作，只能将数据保存到内存中，如果想保存到磁盘文件，需要更改存储级别

 注意：
 1、RDD 对象的持久化操作不一定是为了重用
 2、在数据执行时间或者执行阶段较长，或者数据比较重要的场合也可以采用持久化操作

 1、RDD Cache缓存
  RDD 通过 Cache 或者 Persist 方法将前面的计算结果缓存，默认情况下会把数据缓存在JVM的堆内存中。但是并不是这两个方法被调用时立即缓存，而是触发后面的action算子时，该RDD将会被缓存在计算节点的内存中，并供后面重用。

  存储级别:

  DISK_ONLY 存储磁盘
  DISK_ONLY_2 存储磁盘，并且有副本
  MEMORY_AND_DISK 存储内存，内存放不下，溢写磁盘
  MEMORY_AND_DISK_2 存储内存，内存满了，溢写磁盘，并且有副本
  MEMORY_ONLY 存储内存, 内存放不下，不会溢写磁盘
  MEMORY_ONLY_2 存储内存, 内存放不下，不会溢写磁盘，并且有副本
  MEMORY_ONLY_SER 仅在内存存储序列化数据
  MEMORY_ONLY_SER_2 仅在内存存储序列化后的数据，并且有副本
  MEMORY_AND_DISK_SER 如果数据在内存中放不下，则溢写磁盘。在内存存储序列化后的数据
  MEMORY_AND_DISK_SER_2 如果数据在内存中放不下，则溢写磁盘。在内存存储序列化后的数据，并且有副本

 2、RDD CheckPoint 检查点

 所谓的检查点其实就是通过将RDD中间结果写入磁盘
 由于血缘依赖过长会造成容错成本过高，这样就不如在中间阶段做检查点容错，如果检查点之后有节点出现问题，可以从检查点开始重做血缘，减少开销。

 对RDD 进行checkpoint操作并不会马上被执行，必须执行Action操作才能触发。

 1、// 设置 checkpoint 路径
    sc.setCheckpointDir("cp")

 2、mapRDD.cache()
    mapRDD.checkpoint()


注意：
 checkpoint 需要落盘，那就需要指定检查点保存路径
 检查点路径中保存文件，当作业执行完成时，不会被删除
 一般保存路径都是在分布式存储系统 ：HDFS

 cache 、persist 和 checkpoint之间的区别？

 cache：将数据临时存储在内存中进行数据重用
 			会在血缘关系中添加新的依赖,一旦出现问题，可以重头读取数据
 persist：将数据临时存储在磁盘文件中进行数据重用
 			涉及到磁盘IO，性能较低，但是数据安全
 			如果作业执行完毕，临时保存的数据文件就会丢失
 checkpoint：将数据长久保存在磁盘文件中进行数据重用
 			涉及到磁盘IO，性能较低，但是数据安全
 			为了保证数据安全，所以一般情况下，会独立执行作业（另外再执行一次）。
 			为了能够提高效率，一般情况下，是需要我们和cache联合使用的
 			执行过程中，会切断血缘关系，重新建立新的血缘关系
 			checkpoint等同于改变数据源



RDD 分区器

Spark 目前支持 Hash 分区 和 Range 分区，和自定义分区。Hash分区为当前的默认分区。分区器直接决定了RDD中分区个数、RDD中每条数据经过 Shuffle后进入哪个分区，进而决定了 Reduce 个数。

 1、只有 Key - Value 类型的 RDD 才有分区器，非Key - Value类型的RDD分区的值是None
 2、每个 RDD 的分区 ID 范围：0～（numPartitions - 1）,决定这个值是属于哪个分区的。

1）、Hash 分区：对于给定的key，计算其 hashCode 并除以分区个数取余
2)、Range 分区

自定义分区器：
1、继承 Partitioner
2、重写方法


文件的保存和读取
save：
    rdd.saveAsTextFile("output")
    rdd.saveAsObjectFile("output1")
    rdd.saveAsSequenceFile("output2")
load：
    val rdd = sc.textFile("output")
    println(rdd.collect().mkString(","))
    val rdd1 = sc.objectFile[(String, Int)]("output1")
    println(rdd1.collect().mkString(","))
    val rdd2 = sc.sequenceFile[String, Int]("output2")
    println(rdd2.collect().mkString(","))




累加器：分布式共享 只写 变量

实现原理：
累加器用来把 Executor 端变量信息聚合到 Driver 端。在 Driver 程序中定义的变量，在 Executor 端的每个 Task 都会得到这个变量的一份新的副本，每个 task 更新这些副本的值后，传回 Driver 端进行 merge。

系统累加器：
Spark 默认就提供了简单数据聚合的累加器，如下：
val sumAcc = sc.longAccumulator 
sc.doubleAccumulator
sc.collectionAccumulator
// 使用累加器
sumAcc.add(num)
// 获取累加器的值
sumAcc.value


自定义数据累加器：
// 1、继承 AccumulatorV2,定义泛型
   // IN：累加器输入的数据类型 String
   // OUT：累加器输出的数据类型 mutable.Map[String, Long]
  // 2、重写方法

使用自定义数据累加器的步骤：
1、// 创建累加器对象
    val accumulator = new MyAccumulator()
2、// 向Spark 进行注册
    sc.register(accumulator, "wordCountAcc")

3、// 在行动算子中使用累加器
        accumulator.add(word)
4、// 获取累加器累加的结果
    println(accumulator.value)



注意：
1、少加： 转换算子中调用累加器，如果没有行动算子的话，那么不会执行
2、多加： 转换算子中调用累加器，如果有多个行动算子的话，那么会执行多次
3、一般情况下，累加器会放置在行动算子中进行操作



广播变量：分布式共享 只读 变量

实现原理：广播变量用来高效分发较大对象。向所有工作节点发送一个较大的只读值，以供一个或多个Spark 操作使用。比如，如果你的应用需要向所有节点发送一个较大的只读查询表，广播变量用起来就很顺手。在多个并行操作中使用同一个变量，但是 Spark 会为每个任务分别发送。


    // 闭包数据，都是以Task 为单位发送的，每个任务中包含闭包数据。
    // 这样可能会导致，一个Executor中含有大量重复数据，并且占用大量的内存

    // Executor其实就是一个JVM，所以在启动时，会自动分配内存
    // 完全可以将任务中的闭包数据放置在Executor的内存中，达到共享的目的。

    // Spark中的广播变量，就可以将闭包数据保存到Executor的内存中
    // Spark 的广播变量不能够更改：分布式共享只读变量

