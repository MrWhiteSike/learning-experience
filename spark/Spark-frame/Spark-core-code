Spark核心编程

Spark计算框架为了能够进行高并发和高吞吐的数据处理，封装了三大数据结构，用于处理不同的应用场景，三大数据结构分别是：
RDD：弹性分布式数据集
累加器：分布式共享只写变量
广播变量：分布式共享只读变量


RDD，Resilient Distributed Dataset
弹性分布式数据集，是spark中最基本的数据处理模型。最小的数据处理单元。
代码中是一个抽象类，代表一个弹性的、不可变、可分区、里面的元素可并行计算的集合。
是一种数据准备好，逻辑准备好的数据结构。组织、存储的结构

弹性：
存储的弹性：内存与磁盘的自动切换
容错的弹性：数据丢失可以自动恢复
计算的弹性：计算出错重试机制
分片的弹性：可根据需要重新分片

在Driver中把RDD分解成不同的Task发给Executor去执行计算。


################### IO模型类比RDD ###################

IO模型中：
字节流：提升性能的方式是：添加一个缓冲区buffer，当数据达到阈值时，才会输出到下游。
字符流：一行一行读，需要用到字节流转字符流的转换流InputStreamReader/OutputStreamWriter

装饰者设计模式，核心没变，在外层进行了包装，在之前的基础上扩展更强大的功能。
FileInputStream 读取字节-- InputStreamReader 字节转字符 -- BufferedFileReader 缓存字符
逐层功能的叠加
延迟加载的感觉


WordCount：

file --> HadoopRDD-textFile --> MapPartitionsRDD-flatMap --> MapPartitionsRDD-map --> ShuffledRDD-reduceByKey --> collect --> console

RDD与IO流的异同：

RDD的数据处理方式类似于IO流，也有装饰者设计模式。
RDD的数据只有在调用collect方法时，才会真正执行业务逻辑操作，之前的封装全部都是功能的扩展。
RDD是不保存数据的，但是IO可以临时保存一部分数据。


分布式：数据存储在大数据集群不同节点上
数据集：RDD封装了计算逻辑，并不保存数据
数据抽象：RDD是一个抽象类，需要子类具体实现
不可变：RDD封装了计算逻辑，是不可以改变的，想要改变，只能产生新的的RDD，在新的RDD里面封装计算逻辑
可分区、并行计算

RDD 核心属性
1、分区列表
RDD数据结构中存在分区列表，用于执行任务时并行计算，是实现分布式计算的重要属性。
getPartitions

2、分区计算函数
Spark在计算时，是使用分区函数对每一个分区进行计算
compute

3、RDD之间的依赖关系
RDD是计算模型的封装，当需求中需要将多个计算模型进行组合时，就需要将多个RDD建立依赖关系，就会形成一个列表
getDependices

4、分区器Partitioner
分区规则

5、首选位置
判断计算发送到哪个节点，效率最优
getPreferredLocations

移动数据不如移动计算。


################### 执行原理 ###################

执行原理
从计算角度来讲，数据处理过程需要计算资源（内存 & CPU）和计算模型（逻辑）。执行时，需要将计算资源和计算模型进行协调和整合。

Spark 框架在执行时，先申请资源，然后将应用程序的数据处理逻辑分解成一个一个计算任务，然后将任务发到已经分配好资源的计算节点上，按照指定的计算模型进行数据计算。最后得到计算结果。


RDD是Spark框架中用于数据处理的核心模型。在Yarn环境中，RDD的工作原理：

RDD在整个流程中主要用于将逻辑进行封装，并生成Task发送给Executor节点执行计算。


################### RDD 创建方式 ###################

RDD创建：
1、从集合（内存）中创建
从集合中创建RDD，Spark主要提供了两个方法：parallelize和makeRDD
2、从外部存储（文件）创建RDD
textFile : 以行为单位来读取数据
wholeTextFiles ： 以文件为单位读取数据
    读取的结果表示为元组，第一个参数表示文件路径，第二个参数为文件内容
注意：path参数
1、path路径默认以当前环境的根路径为基准。可以写绝对路径，也可以写相对路径
2、path路径可以是文件的具体路径，也可以是目录名称
3、path路径还可以使用通配符 *
4、path路径还可以是分布式存储系统路径：HDFS
3、从其他框架创建
4、new创建



RDD并行度与分区：
默认情况下，Spark可以将一个作业切分为多个任务，发送给Executor节点进行计算，而能够并行计算的任务数量我们称之为并行度。
这个数量可以在构建RDD时指定。记住，这里的并行执行的任务数量，并不是指的切分任务的数量。

例如，makeRDD方法
1、可以传递第二个参数，这个参数表示分区的数量
2、第二个参数是可以不传递，会使用默认值：defaultParallelism
Spark在默认情况下，从配置对象中获取配置参数:spark.default.parallelism
如果获取不到，那么使用totalCores属性，这个属性取值为当前运行环境的最大可用核数。

分区源码解析：
 1、调用了 ParallelCollectionRDD.slice[T : ClassTag](seq: Seq[T], numSlices: Int): Seq[Seq[T]] 方法
 2、接着调用def positions(length: Long, numSlices: Int): Iterator[(Int, Int)] 方法，通过分区数获取每个分区在seq中的（start，end）元素位置
 3、然后通过（start，end）切割出seq中对应位置的元素，完成分区操作



 读取文件数据时，数据是按照Hadoop文件读取的规则进行切片分区，而切片规则和数据读取的规则有些差异：
textFile : 可以将文件作为数据处理的数据源，默认也可以设定分区。
      minPartitions ： 最小分区数量
      math.min(defaultParallelism, 2) 默认 2 个分区
如果不想使用默认的分区数量，可以通过第二个参数指定分区数
 注意：Spark 读取文件，底层其实使用的是Hadoop的读取方式
 分区数量的计算方式：
 	  假设要读取文件的大小为23byte，指定分区数为3时，即：
      totalSize = 23
      计算规则：
      goalSize = 23 / 3 = 7 (byte)
      23 / 7 = 3.2..（>10%）+ 1 = 4 (分区) (Hadoop 读取文件时，有个1.1 倍概念：如果剩余的字节数/每个分区字节数 > 10%时，产生一个新分区；如果 < 10%，不会产生新的分区)

分区数据的分配：
  1、数据以行为单位进行读取，和字节数没有关系
  2、数据读取时是以偏移量为单位，偏移量不会被重复读取。
  3、数据分区的偏移量范围的计算：
     goalSize = 23 / 3 = 7 (byte)
     [0,7] 不够一行的，读取整行；
     [7,14] 由于不能重复读取，开始读取下一行，但是数据也是不够一行，也是读取整行；数据读取完了
     [14,21] 该分区没有数据
     [21,23] 该分区没有数据

 如果数据源为多个文件，那么计算分区时以文件为单位进行分区


################### RDD 两种类型的方法：Operator-Transform（转换）和  Operator-Action（行动） ###################


RDD方法分类：两大类
1、转换，功能的补充，将旧的RDD包装成新的RDD
	flatMap, map

2、行动，触发任务的调度和作业的执行
	collect


RDD 方法 ==> RDD 算子
认知心理学认为解决问题其实将问题的状态进行改变：
问题（初始）=> 操作（算子） => 问题（审核中）=> 操作（算子） => 问题（完成）

操作：Operator


################### Operator-Transform : ###################

RDD转换算子：
RDD根据数据处理方式的不同将算子整体上分为 Value类型、双Value类型和Key-Value类型

Value 类型

map：将处理的数据逐条进行映射转换，这里的转换可以是类型的转换，也可以是值的转换。

注意：
 1、RDD的计算一个分区内的数据是一个一个执行逻辑
    -- 只有前面一个数据全部的逻辑执行完毕后，才会执行下一个数据
    -- 分区内数据的执行是有序的
 2、不同分区之间数据的执行顺序是无序的


mapPartitions ：可以以分区为单位进行处理，这里的处理是指可以进行任意的处理，哪怕是过滤数据
      但是，会将整个分区的数据加载到内存进行引用
      如果处理完的数据是不会被释放的，存在对象的引用
      在内存较小，数据量较大的场合下，容易出现内存溢出

      还可以实现一些特殊功能，比如取出每个分区中的最大值; map中无法实现，因为map没法区分数据属于哪个分区


问题：map和mapPartitions的区别？

1、数据处理角度：Map算子是分区内一个数据一个数据的执行，类似于串行操作。而mapPartitions算子以分区为单位进行批处理操作

2、功能的角度：
map算子主要目的将数据源中的数据进行转换和改变。但是不会减少或增加数据。mapPartitions算子需要传递一个迭代器，返回一个迭代器，没有要求元素的个数保持不变，可以增加或减少数据

3、性能的角度：
Map算子因为类似于串行操作，所以性能比较低，而mapPartitions算子类似于批处理，所以性能比较高。但是mapPartitions算子会长时间占用内存，那么这样会导致内存可能不够用，出现内存溢出错误。所以在内存有限的情况下，不推荐使用mapPartitions，而是使用map操作。

完成比完美更重要。

mapPartitionsWithIndex: 可以以分区为单位发送到计算节点进行处理，这里的处理是指可以进行任意的处理，哪怕是过滤数据，在处理时同时可以获取当前分区索引。

特殊功能：比如，获取某个分区的数据、某个数据对应的所属分区等功能


flatMap：将处理的数据进行扁平化后再进行映射处理，所以算子也称为扁平映射。返回一个可迭代的集合就是满足要求的。

注意：当集合中的数据类型不同时，可以使用match case 进行模式匹配，转换成集合类型返回。



glom：将同一个分区的数据直接转换为相同类型的内存数组进行处理，分区不变。


groupBy：将数据根据指定的规则进行分组，分区默认不变，但是数据会被打乱，我们将这样的操作称为shuffle，极限情况下，数据可能会在同一个分区中。

一个组的数据在一个分区中，但是并不是说一个分区中只有一个组。

filter：将数据根据指定的规则进行筛选过滤，符合规则的数据保留，不符合规则的数据丢弃。
当数据进行筛选过滤后，分区不变，但是分区内数据可能不均衡，生产环境中，可能会出现数据倾斜。


sample：根据指定的规则从数据集中抽取数据
需要传递三个参数:
 第一个参数：表示抽取数据后是否将数据放回 true：放回，false：不放回
 第二个参数：表示数据源中每条数据被抽取的概率
 			基准值的概念
 第三个参数：表示抽取数据时随机算法的种子
 			如果不传递第三个参数，那么使用的是当前系统时间作为随机数的种子

使用场景：
数据倾斜时，通过sample找到倾斜的key，然后针对这个key进行特殊处理，避免数据倾斜。


distinct：将数据集中重复数据去重

内存集合distinct的去重方式使用 HashSet 去重
RDD的distinct底层逻辑：map(x => (x, null)).reduceByKey((x, _) => x, numPartitions).map(_._1)，使用分布式处理方式实现去重

如果不用该算子，有什么办法实现数据去重？
groupBy 分组实现
filter 过滤实现


coalesce: 根据数据量缩减分区，用于大数据集过滤后，提高数据集的执行效率
当Spark程序中存在过多的小任务时，可以通过coalesce方法，收缩合并分区，减少分区个数，减少任务调度成本。

注意：
 1、coalesce方法默认情况下不会将分区数据打乱重新组合
 这种情况下的缩减分区可能会导致数据不均衡，出现数据倾斜
 如果想要让数据均衡，可以进行shuffle处理，添加第二个参数为true就可以了

如何扩大分区呢？

 2、coalesce 算子可以扩大分区的，但是如果不进行shuffle操作，是没有意义的，不起作用的，
 所以如果想要实现扩大分区的效果，需要使用shuffle操作，即设置第二个参数为true

 3、Spark提供了简化的操作
 缩减分区：coalesce，如果想要数据均衡，可以采用shuffle
 扩大分区：repartition 底层代码就是调用 coalesce，而且肯定shuffle


sortBy: 根据指定的规则进行排序，默认为升序，第二个参数可以改变排序方式
		sortBy 默认情况下，不会改变分区，但是中间存在shuffle操作


双 Value 类型：

intersection : 对源RDD和参数RDD求交集后返回一个新的RDD

问题：如果两个RDD数据类型不一致怎么办？

union: 	求并集

subtract: 求差集

zip：将两个RDD中的元素，以键值对的形式进行合并。其中，键值对中的key为第一个RDD中的元素，Value为第二个RDD中的相同位置的元素。

注意：
1、交集，并集和差集要求两个数据源数据类型保持一致
2、拉链：两个数据源的数据类型可以不一致，但是，两个数据源的分区数量要保持一致，并且每个分区中数据数量也要保持一致


Key-Value 类型：


partitionBy：将数据按照指定Partitioner重新进行分区。
			Spark默认的分区器是HashPartitioner
问题1: 如果重分区的分区器和当前RDD的分区器一样怎么办？
不会生成新的RDD，还是当前RDD本身

问题2: Spark还有其他分区器吗？
HashPartitioner 默认情况下的分区器
RangePartitioner 排序的场景下用的比较多
PythonPartitioner 特定包下才可以用的

问题3:如果想按照自己的方法进行数据分区怎么办？
自定义分区器来改变数据存放的位置


reduceByKey ： 可以将数据按照相同的Key对Value进行聚合

注意：
	1、scala 语言中一般的聚合都是两两聚合，spark基于scala开发的，所以它的聚合也是两两聚合的
	2、如果key的数据只有一个，是不会参与运算的。

groupByKey： 将数据源中的数据，相同的key的数据分到一个组中，形成一个对偶元组
			元组中的第一个元素就是key，第二个元素就是相同key的value集合


问题：reduceByKey 和 groupByKey 的区别？

groupByKey 会导致数据打乱重组，存在shuffle操作。

Spark 中，shuffle操作必须落盘处理，不能在内存中数据等待，会导致内存溢出。shuffle操作的性能比较低。

reduceByKey ：支持分区内预聚合（combine）功能，可以有效减少shuffle时落盘的数据量，提升shuffle的性能。
分区内和分区间计算规则是相同的。

从shuffle的角度： reduceByKey 和 groupByKey 都存在shuffle操作，但是reduceByKey可以在shuffle前对分区内相同key的数据进行预聚合（combine）功能，这样会减少落盘的数据量，而 groupByKey 只是进行分组，不存在数据量减少的问题，reduceByKey 性能更高
从功能的角度： reduceByKey 其实包含分组和聚合的功能。 groupByKey 只能分组，不能聚合，所以在分组聚合的场合下，推荐使用 reduceByKey ，如果仅仅是分组而不需要聚合。那么还是只能使用 groupByKey



aggregateByKey：可以根据不同的规则进行分区内计算和分区间计算。存在函数柯里化，有两个参数列表
 第一个参数列表:需要传递一个参数，表示为初始值
      主要用于当碰见第一个key的时候，和value进行分区内计算
 第二个参数列表：
      第一个参数表示分区内计算规则
      第二个参数表示分区间计算规则


foldByKey: 如果聚合计算时，分区内和分区间计算规则相同，Spark提供了简化的方法
	第一个参数列表:需要传递一个参数，表示为初始值
      主要用于当碰见第一个key的时候，和value进行分区内计算
    第二个参数列表：
      第一个参数表示分区内和分区间的计算规则


 combineByKey 需要三个参数
	 第一个参数表示：将相同key的第一个数据进行数据结构的转换，实现操作
     第二个参数表示：分区内的计算规则
     第三个参数表示：分区间的计算规则

问题：几种聚合算子的区别？

reduceByKey :
  分区内和分区间计算规则相同
  combineByKeyWithClassTag[V](
  (v: V) =>
   v,   // 第一个值不会参与计算
   func, // 分区内计算规则
   func, // 分区间计算规则
   partitioner)

 aggregateByKey :
  分区内和分区间计算规则不同
  combineByKeyWithClassTag[U](
 (v: V) =>
  cleanedSeqOp(createZero(), v), // 初始值和第一个key的value值进行的分区内数据操作
  cleanedSeqOp, // 分区内计算规则
 combOp, // 分区间计算规则
 partitioner)



 foldByKey :
  分区内和分区间处理函数相同
 combineByKeyWithClassTag[V]((v: V) =>
 cleanedFunc(createZero(), v), // 初始值和第一个key的value值进行的分区内数据操作
 cleanedFunc, // 表示分区内数据的处理函数
 cleanedFunc, // 表示分区间数据的处理函数
 partitioner)


 combineByKey :
  分区内和分区间处理函数不同
 combineByKeyWithClassTag(
 createCombiner, // 相同的key的第一条数据进行的处理函数
 mergeValue, // 表示分区内数据的处理函数
 mergeCombiners, // 表示分区间数据的处理函数
 defaultPartitioner(self))



join ：在类型为（K，V）和（K，W）的RDD上调用，返回一个相同的key对应的所有元素连接在一起的（K，（V，W））的RDD

相同于SQL表中的内连接

问题：如果key存在不相等呢？
注意：
1、如果两个数据源中key没有匹配上，那么数据不会出现在结果中。
2、如果两个数据源中key有多个相同的，会依次匹配，可能会出现笛卡尔积，数据量会几何性增长，会导致性能降低；如果有其他办法代替，最好不要用这个算子。
谨慎使用，如果不用join也能实现同样的功能，那就好了！


leftOuterJoin: 类似于SQL语句的左外连接，左表所有的元素都会出现，右表的匹配元素出现

rightOuterJoin：类似于SQL中右外连接；右表所有的元素都会出现，左表的匹配元素出现



cogroup: connect + group
相同的key，value分组后连接起来；

该算子可以最多连接三个其他的rdd。

################### Operator-Action （行动）: ###################
行动算子，其实就是触发作业执行的方法
底层代码逻辑：调用的是环境对象中 runJob 方法，然后调用dagScheduler（有向无环图）的runJob方法创建ActiveJob，并提交执行。


reduce：
聚合RDD中的所有数据，先聚合分区内数据，在聚合分区间数据


collect : 
采集，该方法会将不同分区间的数据按照分区顺序采集到Driver端，形成数组


count : 
数据源中数据个数


first : 
获取数据源中数据的第一个


take : 
获取数据源中数据的前N个

takeOrdered : 
数据先排序，再取N个，默认升序排序，可以使用第二个参数列表（比如 ： Ordering.Int.reverse）实现倒序功能


aggregate : 分区的数据通过 初始值 和分区内的数据进行聚合，然后再和 初始值 进行分区间的数据聚合

aggregateByKey 和 aggregate 的区别？
aggregateByKey ：初始值只会参与分区内计算；转换算子，得到新的RDD
aggregate ：初始值会参与分区内计算，并且还参与分区间计算；行动算子，得到结果


fold ：
折叠，aggregate 简化版，分区内和分区间计算规则相同时


countByValue :
统计数据源中value出现的个数（Long），返回一个Map，value值作为Map的key，个数作为Map的value


countByKey :
要求数据为 Key-Value 类型，统计数据源中 key 出现的个数（Long），返回一个Map，数据源中的 key 值作为Map的key，个数作为Map的value


save方法：
saveAsTextFile
saveAsObjectFile
saveAsSequenceFile ： 方法要求数据的格式必须为 K-V 键值对类型


foreach :
分布式遍历 RDD中的每一个元素，调用指定函数。

	总结 
    // 算子 ： Operator （操作）
    //        RDD的方法和Scala集合对象的方法不一样：
    //        集合对象的方法都是在同一个节点的内存中完成的
    //        RDD方法可以将计算逻辑发送到Executor端（分布式节点）执行的
    // 		  为了区分不同的处理效果，所以将RDD方法称之为算子
    //		  RDD 方法外部的操作都是在Driver端执行的，而方法内部的逻辑代码是在Executor端执行的。



RDD 序列化：

1、闭包检测
 从计算角度，算子以外的代码都是在Driver端执行，算子里面的代码都是在Executor端执行。那么在scala的函数式编程中，就会导致算子内经常会用到算子外的数据，这样就会形成闭包效果，如果使用的算罪外的数据无法序列化，就意味着无法传值给Executor端执行，就会发生错误，所以需要在执行任务计算前，检测闭包内的对象是否可以进行序列化，这个操作我们称之为 闭包检测。
 Scala2.12版本后闭包编译方式发生了改变。

 2、序列化方法和属性

 















































