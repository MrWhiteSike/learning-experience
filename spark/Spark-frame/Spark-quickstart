
SparkVsHadoop

1、Spark 和 Hadoop的根本差别是多个作业之间的数据通信问题：
spark 多个作业之间数据通信是基于内存的，而Hadoop是基于磁盘

2、Spark Task启动时间快，Spark采用fork线程方式，而Hadoop采用创建进程的方式

3、spark只有在shuffle的时候将数据写入磁盘，而Hadoop中多个MR作业之间的数据交互都要依赖于磁盘交互

4、spark缓存机制比HDFS缓存机制高效

spark是基于内存的，所以在实际的生产环境中，由于内存的限制，可能会由于内存资源不够导致job执行失败，此时MapReduce其实是一个更好的选择，所以Spark并不能完全替代MR。



Spark 核心模块

Spark Core 基础核心
提供了Spark最基础与最核心的功能，下面四种功能模块都是在core基础上进行扩展的。

Spark SQL 结构化操作
是Spark用来操作结构化数据的组件，用户可以使用SQL或者Hive 版本的SQL方言（HQL）来查询数据。

Spark Streaming 流式操作
是Spark平台上针对实时数据进行流式计算的组件，提供了丰富的处理数据流的API。


Spark ML 机器学习
是Spark提供的一个机器学习算法库，不仅提供了模型评估、数据导入等额外的功能，还提供了一些更底层的机器学习原语。

Spark GraphX 图
是Spark面向图计算提供的框架与算法库。


打印日志：

在resources目录下添加log4j.properties文件


为了避免大量日志打印，显示混乱问题，可添加Error级别的日志配置：
log4j.rootLogger=ERROR, console
log4j.appender.console=org.apache.log4j.ConsoleAppender
log4j.appender.console.layout=org.apache.log4j.PatternLayout
log4j.appender.console.layout.ConversionPattern=%d{HH:mm:ss,SSS} %-5p %-60c %x - %m%n





