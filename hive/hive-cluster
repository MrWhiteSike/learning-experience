hive-cluster

初始准备：
准备3台虚拟机
192.168.36.121 hadoop1  在这个服务器上安装MySQL
192.168.36.122 hadoop2
192.168.36.123 hadoop3

需要安装好jdk，环境生效即可
需要配置好hadoop并启动
需要安装并配置MySQL


安装Hive
注意：Hive安装在一个节点就可以了

1、下载
wget https://mirrors.tuna.tsinghua.edu.cn/apache/hive/hive-3.1.2/apache-hive-3.1.2-bin.tar.gz
2、解压
tar -zxvf apache-hive-3.1.2-bin.tar.gz
mv apache-hive-3.1.2-bin hive
3、修改Hive的核心配置，选择远程MySQL模式(Hive有三种运行模式)
cd /opt/module/hive/conf
cp hive-default.xml.template hive-site.xml
vim hive-site.xml

修改如下核心配置：


<property>
        <name>javax.jdo.option.ConnectionURL</name>
        <value>jdbc:mysql://hadoop1:3306/hivedb?createDatabaseIfNotExist=true&amp;characterEncoding=UTF-8&amp;useSSL=false&amp;serverTimezone=GMT</value>
</property>

<property>
        <name>javax.jdo.option.ConnectionDriverName</name>
        <value>com.mysql.cj.jdbc.Driver</value>
</property>

## 你的Mysql账号
<property>
        <name>javax.jdo.option.ConnectionUserName</name>
        <value>root</value>
</property>

## 你的Mysql密码
<property>
        <name>javax.jdo.option.ConnectionPassword</name>
        <value>123456</value>
</property>

## 忽略HIVE 元数据库版本的校验，如果非要校验就得进入MYSQL升级版本
<property>
        <name>hive.metastore.schema.verification</name>
        <value>false</value>
</property>

<property> 
     <name>hive.cli.print.current.db</name>
     <value>true</value>
</property>

<property> 
         <name>hive.cli.print.header</name>
         <value>true</value>
</property>

<!-- hiveserver2 -->
<property>
         <name>hive.server2.thrift.port</name>
         <value>10000</value>
</property>

<property>
	<name>hive.server2.thrift.bind.host</name>
	<value>hadoop1</value>
</property>

mkdir /opt/module/hive/tmp
然后把所有${system:java.io.tmpdir} 替换成固定的目录 /opt/module/hive/tmp


4、下载连接MySQL的驱动包到hive的lib目录下
wget https://repo1.maven.org/maven2/mysql/mysql-connector-java/8.0.17/mysql-connector-java-8.0.17.jar

5、在MySQL上创建Hive的元数据存储库
create database hivedb;

6、执行Hive的初始化工作：
hive/bin/schematool -initSchema -dbType mysql


7、初始化完成后，在MySQL的hivedb数据库中查看是否初始化成功
show tables;

若展示多个数据表，即代表初始化成功

8、使用hive
使用beeline来使用HIVE，注意若你要使用MR，则可以使用hive。但在新版本中推荐使用beeline,而beeline内置使用了Spark。

首先启动hiveserver2并令其处于挂起状态：
nohup ./hiveserver2>> hiveserver2.log 2>&1 &

然后使用beeline客户端访问hiveserver2服务
./beeline
beeline> !connect jdbc:hive2://hadoop1:10000/default



如果不使用beeline也可以直接hive命令，操作hive（引擎为MR）。




问题1：hive启动报错
com.google.common.base.Preconditions.checkArgument(ZLjava/lang/String;Ljava/lang/Object;)V
	at org.apache.hadoop.conf.Configuration.set(Configuration.java:1357)
	at org.apache.hadoop.conf.Configuration.set(Configuration.java:1338)
	at org.apache.hadoop.mapred.JobConf.setJar(JobConf.java:518)
	at org.apache.hadoop.mapred.JobConf.setJarByClass(JobConf.java:536)
	at org.apache.hadoop.mapred.JobConf.<init>(JobConf.java:430)
	at org.apache.hadoop.hive.conf.HiveConf.initialize(HiveConf.java:5141)
	at org.apache.hadoop.hive.conf.HiveConf.<init>(HiveConf.java:5104)
	at org.apache.hive.beeline.HiveSchemaTool.<init>(HiveSchemaTool.java:96)
	at org.apache.hive.beeline.HiveSchemaTool.main(HiveSchemaTool.java:1473)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.util.RunJar.run(RunJar.java:318)
	at org.apache.hadoop.util.RunJar.main(RunJar.java:232)

错误原因：系统找不到这个类所在的jar包或者jar包的版本不一样系统不知道使用哪个。hive启动报错的原因是后者

解决办法：
1、com.google.common.base.Preconditions.checkArgument这个类所在的jar包为：guava.jar
2、hadoop-3.1.3（路径：hadoop\share\hadoop\common\lib）中该jar包为  guava-27.0-jre.jar；而hive-3.1.2(路径：hive/lib)中该jar包为guava-19.0.jar
3、将jar包变成一致的版本：删除hive中低版本jar包，将hadoop中高版本的复制到hive的lib中。
再次启动问题得到解决！


问题2：
Windows10 系统启动Hive3 提示如下错误信息：
com.ctc.wstx.exc.WstxParsingException: Illegal character entity: expansion character (code 0x8
 at [row,col,system-id]: [3215,96,"file:/opt/module/hive/conf/hive-site.xml"]
	at org.apache.hadoop.conf.Configuration.loadResource(Configuration.java:3024)
	at org.apache.hadoop.conf.Configuration.loadResources(Configuration.java:2973)
	at org.apache.hadoop.conf.Configuration.getProps(Configuration.java:2848)
	at org.apache.hadoop.conf.Configuration.get(Configuration.java:1460)
	at org.apache.hadoop.hive.conf.HiveConf.getVar(HiveConf.java:4996)
	at org.apache.hadoop.hive.conf.HiveConf.getVar(HiveConf.java:5069)
	at org.apache.hadoop.hive.conf.HiveConf.initialize(HiveConf.java:5156)
	at org.apache.hadoop.hive.conf.HiveConf.<init>(HiveConf.java:5104)
	at org.apache.hive.beeline.HiveSchemaTool.<init>(HiveSchemaTool.java:96)
	at org.apache.hive.beeline.HiveSchemaTool.main(HiveSchemaTool.java:1473)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.util.RunJar.run(RunJar.java:318)
	at org.apache.hadoop.util.RunJar.main(RunJar.java:232)

造成问题的原因：hive/conf/hive-site.conf 配置文件的3215行的96个字符包含特殊字符 #8

解决：将相关特殊字符注释即可 



问题3：Error: Could not open client transport with JDBC Uri: jdbc:hive2://hadoop1:10000/test: Failed to open new session: java.lang.RuntimeException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.security.authorize.AuthorizationException): User: root is not allowed to impersonate anonymous (state=08S01,code=0)

参考：https://blog.csdn.net/wuleidaren/article/details/106395543

解决办法：
1、修改hadoop 配置文件 etc/hadoop/core-site.xml,加入如下配置项
<property>     <name>hadoop.proxyuser.root.hosts</name>     <value>*</value> </property> <property>     <name>hadoop.proxyuser.root.groups</name>     <value>*</value> </property>
hadoop.proxyuser.root.hosts 配置项名称中root部分为报错User:* 中的用户名部分；
如果是其他用户报错如下：
例如User: zhaoshb is not allowed to impersonate anonymous则需要将xml变更为如下格式
<property>     <name>hadoop.proxyuser.zhaoshb.hosts</name>     <value>*</value> </property> <property>     <name>hadoop.proxyuser.zhaoshb.groups</name>     <value>*</value> </property>

2、重启hadoop stop-dfs.sh  start-dfs.sh
3、测试
./beeline -u 'jdbc:hive2://localhost:10000/userdb' -n root

或者
./beeline
beeline> !connect jdbc:hive2://hadoop1:10000/test

Connecting to jdbc:hive2://hadoop1:10000/test
Enter username for jdbc:hive2://hadoop1:10000/test: root            #输入用户名
Enter password for jdbc:hive2://hadoop1:10000/test: ******          #输入密码

出现如下信息，表示连接成功：
Connected to: Apache Hive (version 3.1.2)
Driver: Hive JDBC (version 3.1.2)
Transaction isolation: TRANSACTION_REPEATABLE_READ
0: jdbc:hive2://hadoop1:10000/test>



